# StatQuest机器学习系列 - 回归树详解

1. 什么是回归树？它与分类树有什么区别？
回归树是一种特殊的决策树，用于预测连续数值。
- 回归树的叶节点代表数值型预测值（如药物有效性数值）
- 分类树的叶节点代表类别（如真/假、猫/狗）
- 两者的主要区别在于预测目标的类型：连续值 vs 离散类别

2. 为什么需要回归树而不是线性回归？
在以下情况下回归树比线性回归更有优势：
- 数据关系呈非线性特征（如U型曲线）
- 变量之间的关系复杂，难以用简单数学公式描述
- 需要处理多个预测变量时，回归树能更好地适应数据特征

3. 回归树是如何构建的？
回归树采用自上而下的构建方式：
- 寻找最佳分割点：计算所有可能分割点的残差平方和(SSR)
- 选择最优分割：选择SSR最小的分割点作为节点
- 递归分割：对子节点重复上述过程
- 停止条件：达到最小样本数或其他终止条件

4. 什么是残差平方和(SSR)？它的作用是什么？
SSR是评估模型预测效果的重要指标：
- 计算方法：所有数据点的（观察值-预测值）的平方和
- 作用：用于选择最佳分割点
- 特点：SSR越小，表示预测效果越好

5. 如何处理多个预测变量的情况？
处理多个预测变量的策略：
- 对每个变量计算所有可能的分割点的SSR
- 在所有变量的所有分割点中选择SSR最小的那个
- 选定的变量和分割点作为当前节点的分割标准

6. 如何防止回归树过拟合？
主要有以下几种防止过拟合的方法：
- 限制树的深度
- 设置最小叶节点样本数（通常为20）
- 使用剪枝技术删除不必要的分支
- 通过交叉验证选择最优参数

7. 叶节点的预测值如何确定？
叶节点预测值的确定方法：
- 计算该节点所有样本的响应变量平均值
- 将该平均值作为该叶节点的预测值
- 对新数据，使用其所属叶节点的平均值作为预测结果

8. 什么时候停止节点分割？
满足以下任一条件时停止分割：
- 节点样本数小于预设阈值（如20）
- 达到最大树深度
- 节点内样本的响应变量值相同
- 分割后SSR的减少量低于阈值

> 补充说明：回归树是一种灵活的预测模型，特别适合处理非线性关系和复杂数据结构。通过合理的参数设置和防过拟合措施，可以构建出稳健的预测模型。